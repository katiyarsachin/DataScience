{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import all required libraries\n",
    "import numpy as np\n",
    "import os\n",
    "from scipy.misc import imread, imresize, imshow\n",
    "import datetime\n",
    "import os\n",
    "import warnings\n",
    "from PIL import Image \n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#import tensorflow and setting seed value\n",
    "np.random.seed(30)\n",
    "import random as rn\n",
    "rn.seed(30)\n",
    "from keras import backend as K\n",
    "import tensorflow as tf\n",
    "tf.set_random_seed(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing train and val CSV files and setting batch size as 15\n",
    "train_doc = np.random.permutation(open('Project_data/train.csv').readlines())\n",
    "val_doc = np.random.permutation(open('Project_data/val.csv').readlines())\n",
    "batch_size = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generator function with batch size=15, img_idx = [0,2,4....,18], image size = (100,100), also covered with remaining data points\n",
    "def generator(source_path, folder_list, batch_size):\n",
    "    print( 'Source path = ', source_path, '; batch size =', batch_size)\n",
    "    img_idx = range(0,30,2)\n",
    "    while True:\n",
    "        t = np.random.permutation(folder_list)\n",
    "        num_batches = int(len(folder_list)/batch_size)\n",
    "        for batch in range(num_batches): # we iterate over the number of batches\n",
    "            batch_data = np.zeros((batch_size,len(img_idx),100,100,3)) # x is the number of images you use for each video, (y,z) is the final size of the input images and 3 is the number of channels RGB\n",
    "            batch_labels = np.zeros((batch_size,5)) # batch_labels is the one hot representation of the output\n",
    "            for folder in range(batch_size): # iterate over the batch_size\n",
    "                imgs = os.listdir(source_path+'/'+ t[folder + (batch*batch_size)].split(';')[0]) # read all the images in the folder\n",
    "                for idx,item in enumerate(img_idx): #  Iterate iver the frames/images of a folder to read them in\n",
    "                    image = imread(source_path+'/'+ t[folder + (batch*batch_size)].strip().split(';')[0]+'/'+imgs[item]).astype(np.float32)\n",
    "                    \n",
    "                    #crop the images and resize them. Note that the images are of 2 different shape \n",
    "                    #and the conv3D will throw error if the inputs in a batch have different shapes\n",
    "                    im = imresize(image,(100,100))\n",
    "                    im1 = np.asarray(im).astype('float32')\n",
    "                    batch_data[folder,idx,:,:,0] = (im1[:,:,0]-im1[:,:,0].min())/(im1[:,:,0].max()-im1[:,:,0].min())\n",
    "                    batch_data[folder,idx,:,:,1] = (im1[:,:,1]-im1[:,:,1].min())/(im1[:,:,1].max()-im1[:,:,1].min())\n",
    "                    batch_data[folder,idx,:,:,2] = (im1[:,:,2]-im1[:,:,2].min())/(im1[:,:,2].max()-im1[:,:,2].min())\n",
    "                    \n",
    "                batch_labels[folder, int(t[folder + (batch*batch_size)].strip().split(';')[2])] = 1\n",
    "            yield batch_data, batch_labels #you yield the batch_data and the batch_labels, remember what does yield do\n",
    "        \n",
    "        # write the code for the remaining data points which are left after full batches\n",
    "        extra=int(len(folder_list))-(num_batches*batch_size)\n",
    "        if extra>0:\n",
    "            batch=num_batches\n",
    "            batch_data = np.zeros((extra,len(img_idx),100,100,3)) # x is the number of images you use for each video, (y,z) is the final size of the input images and 3 is the number of channels RGB\n",
    "            batch_labels = np.zeros((extra,5)) # batch_labels is the one hot representation of the output\n",
    "            for folder in range(extra): # iterate over the batch_size\n",
    "                imgs = os.listdir(source_path+'/'+ t[folder + (batch*batch_size)].split(';')[0]) # read all the images in the folder\n",
    "                for idx,item in enumerate(img_idx): #  Iterate iver the frames/images of a folder to read them in\n",
    "                    image = imread(source_path+'/'+ t[folder + (batch*batch_size)].strip().split(';')[0]+'/'+imgs[item]).astype(np.float32)\n",
    "                   \n",
    "                    #crop the images and resize them. Note that the images are of 2 different shape\n",
    "                    #and the conv3D will throw error if the inputs in a batch have different shapes\n",
    "                    im = imresize(image,(100,100))\n",
    "                    im1 = np.asarray(im).astype('float32')\n",
    "                    batch_data[folder,idx,:,:,0] = (im1[:,:,0]-im1[:,:,0].min())/(im1[:,:,0].max()-im1[:,:,0].min())\n",
    "                    batch_data[folder,idx,:,:,1] = (im1[:,:,1]-im1[:,:,1].min())/(im1[:,:,1].max()-im1[:,:,1].min())\n",
    "                    batch_data[folder,idx,:,:,2] = (im1[:,:,2]-im1[:,:,2].min())/(im1[:,:,2].max()-im1[:,:,2].min())\n",
    "                   \n",
    "                batch_labels[folder, int(t[folder + (batch*batch_size)].strip().split(';')[2])] = 1\n",
    "            yield batch_data, batch_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# training sequences = 663\n",
      "# validation sequences = 100\n",
      "# epochs = 35\n"
     ]
    }
   ],
   "source": [
    "curr_dt_time = datetime.datetime.now()\n",
    "train_path = 'Project_data/train'\n",
    "val_path = 'Project_data/val'\n",
    "num_train_sequences = len(train_doc)\n",
    "print('# training sequences =', num_train_sequences)\n",
    "num_val_sequences = len(val_doc)\n",
    "print('# validation sequences =', num_val_sequences)\n",
    "num_epochs = 35\n",
    "print ('# epochs =', num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing required libraries\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, GRU, Flatten, TimeDistributed, Flatten, BatchNormalization, Activation, Dropout\n",
    "from keras.layers.convolutional import Conv3D, MaxPooling3D\n",
    "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
    "from keras import optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model Building\n",
    "model = Sequential()\n",
    "model.add(Conv3D(8,kernel_size=(3, 3, 3), activation='relu' , input_shape=(15, 100, 100, 3) )) \n",
    "model.add(Conv3D(8,kernel_size=(3, 3, 3), activation='relu', padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling3D(pool_size=(2, 2, 2), strides=(2,2,2)))\n",
    "\n",
    "model.add(Conv3D(16,kernel_size=(3,3,3), activation='relu', padding='same'))  \n",
    "model.add(Conv3D(16,kernel_size=(3,3,3), activation='relu',padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling3D(pool_size=(2, 2, 2), strides=(2,2,2)))\n",
    "\n",
    "model.add(Conv3D(32,kernel_size=(3,3,3), activation='relu', padding='same'))  \n",
    "model.add(Conv3D(32,kernel_size=(3,3,3), activation='relu',padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling3D(pool_size=(2, 2, 2), strides=(2,2,2)))\n",
    "\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(5, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv3d_1 (Conv3D)            (None, 13, 98, 98, 8)     656       \n",
      "_________________________________________________________________\n",
      "conv3d_2 (Conv3D)            (None, 13, 98, 98, 8)     1736      \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 13, 98, 98, 8)     32        \n",
      "_________________________________________________________________\n",
      "max_pooling3d_1 (MaxPooling3 (None, 6, 49, 49, 8)      0         \n",
      "_________________________________________________________________\n",
      "conv3d_3 (Conv3D)            (None, 6, 49, 49, 16)     3472      \n",
      "_________________________________________________________________\n",
      "conv3d_4 (Conv3D)            (None, 6, 49, 49, 16)     6928      \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 6, 49, 49, 16)     64        \n",
      "_________________________________________________________________\n",
      "max_pooling3d_2 (MaxPooling3 (None, 3, 24, 24, 16)     0         \n",
      "_________________________________________________________________\n",
      "conv3d_5 (Conv3D)            (None, 3, 24, 24, 32)     13856     \n",
      "_________________________________________________________________\n",
      "conv3d_6 (Conv3D)            (None, 3, 24, 24, 32)     27680     \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 3, 24, 24, 32)     128       \n",
      "_________________________________________________________________\n",
      "max_pooling3d_3 (MaxPooling3 (None, 1, 12, 12, 32)     0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 1, 12, 12, 32)     0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 4608)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 64)                294976    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 32)                128       \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 5)                 165       \n",
      "=================================================================\n",
      "Total params: 351,901\n",
      "Trainable params: 351,725\n",
      "Non-trainable params: 176\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "#setting optimizer and them compiling the model\n",
    "optimiser = 'adam'\n",
    "model.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "print (model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calling generator function for both train and test data\n",
    "train_generator = generator(train_path, train_doc, batch_size)\n",
    "val_generator = generator(val_path, val_doc, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'model_init' + '_' + str(curr_dt_time).replace(' ','').replace(':','_') + '/'\n",
    "    \n",
    "if not os.path.exists(model_name):\n",
    "    os.mkdir(model_name)\n",
    "        \n",
    "filepath = model_name + 'model-{epoch:05d}-{loss:.5f}-{categorical_accuracy:.5f}-{val_loss:.5f}-{val_categorical_accuracy:.5f}.h5'\n",
    "\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=False, save_weights_only=False, mode='auto', period=1)\n",
    "\n",
    "LR = ReduceLROnPlateau(monitor='loss',\n",
    "                                   factor=0.0001,\n",
    "                                   cooldown=0,\n",
    "                                   patience=5,\n",
    "                                   min_lr=1e-4)\n",
    "callbacks_list = [checkpoint, LR]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (num_train_sequences%batch_size) == 0:\n",
    "    steps_per_epoch = int(num_train_sequences/batch_size)\n",
    "else:\n",
    "    steps_per_epoch = (num_train_sequences//batch_size) + 1\n",
    "\n",
    "if (num_val_sequences%batch_size) == 0:\n",
    "    validation_steps = int(num_val_sequences/batch_size)\n",
    "else:\n",
    "    validation_steps = (num_val_sequences//batch_size) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source path =  Project_data/val ; batch size = 15\n",
      "Source path =  Project_data/train ; batch size = 15\n",
      "Epoch 1/35\n",
      "45/45 [==============================] - 45s 998ms/step - loss: 1.7593 - categorical_accuracy: 0.2978 - val_loss: 5.8642 - val_categorical_accuracy: 0.2476\n",
      "\n",
      "Epoch 00001: saving model to model_init_2020-03-0111_08_09.236698/model-00001-1.75929-0.29778-5.86422-0.24762.h5\n",
      "Epoch 2/35\n",
      "45/45 [==============================] - 41s 910ms/step - loss: 1.4118 - categorical_accuracy: 0.4074 - val_loss: 1.1800 - val_categorical_accuracy: 0.5714\n",
      "\n",
      "Epoch 00002: saving model to model_init_2020-03-0111_08_09.236698/model-00002-1.41181-0.40741-1.18003-0.57143.h5\n",
      "Epoch 3/35\n",
      "45/45 [==============================] - 41s 921ms/step - loss: 1.2314 - categorical_accuracy: 0.5348 - val_loss: 1.1831 - val_categorical_accuracy: 0.5048\n",
      "\n",
      "Epoch 00003: saving model to model_init_2020-03-0111_08_09.236698/model-00003-1.23142-0.53481-1.18308-0.50476.h5\n",
      "Epoch 4/35\n",
      "45/45 [==============================] - 42s 932ms/step - loss: 1.1109 - categorical_accuracy: 0.5363 - val_loss: 1.4250 - val_categorical_accuracy: 0.4190\n",
      "\n",
      "Epoch 00004: saving model to model_init_2020-03-0111_08_09.236698/model-00004-1.11086-0.53630-1.42500-0.41905.h5\n",
      "Epoch 5/35\n",
      "45/45 [==============================] - 41s 912ms/step - loss: 1.0789 - categorical_accuracy: 0.5630 - val_loss: 1.8281 - val_categorical_accuracy: 0.3333\n",
      "\n",
      "Epoch 00005: saving model to model_init_2020-03-0111_08_09.236698/model-00005-1.07887-0.56296-1.82810-0.33333.h5\n",
      "Epoch 6/35\n",
      "45/45 [==============================] - 42s 939ms/step - loss: 0.9495 - categorical_accuracy: 0.6267 - val_loss: 1.2603 - val_categorical_accuracy: 0.4857\n",
      "\n",
      "Epoch 00006: saving model to model_init_2020-03-0111_08_09.236698/model-00006-0.94954-0.62667-1.26029-0.48571.h5\n",
      "Epoch 7/35\n",
      "45/45 [==============================] - 40s 897ms/step - loss: 0.8284 - categorical_accuracy: 0.6800 - val_loss: 0.9656 - val_categorical_accuracy: 0.6286\n",
      "\n",
      "Epoch 00007: saving model to model_init_2020-03-0111_08_09.236698/model-00007-0.82844-0.68000-0.96561-0.62857.h5\n",
      "Epoch 8/35\n",
      "45/45 [==============================] - 42s 924ms/step - loss: 0.7999 - categorical_accuracy: 0.6859 - val_loss: 0.9018 - val_categorical_accuracy: 0.6476\n",
      "\n",
      "Epoch 00008: saving model to model_init_2020-03-0111_08_09.236698/model-00008-0.79994-0.68593-0.90181-0.64762.h5\n",
      "Epoch 9/35\n",
      "45/45 [==============================] - 41s 919ms/step - loss: 0.6996 - categorical_accuracy: 0.7437 - val_loss: 0.8908 - val_categorical_accuracy: 0.6286\n",
      "\n",
      "Epoch 00009: saving model to model_init_2020-03-0111_08_09.236698/model-00009-0.69961-0.74370-0.89081-0.62857.h5\n",
      "Epoch 10/35\n",
      "45/45 [==============================] - 42s 931ms/step - loss: 0.5623 - categorical_accuracy: 0.8252 - val_loss: 1.1007 - val_categorical_accuracy: 0.5810\n",
      "\n",
      "Epoch 00010: saving model to model_init_2020-03-0111_08_09.236698/model-00010-0.56234-0.82519-1.10074-0.58095.h5\n",
      "Epoch 11/35\n",
      "45/45 [==============================] - 42s 934ms/step - loss: 0.5144 - categorical_accuracy: 0.8267 - val_loss: 0.9287 - val_categorical_accuracy: 0.6381\n",
      "\n",
      "Epoch 00011: saving model to model_init_2020-03-0111_08_09.236698/model-00011-0.51440-0.82667-0.92866-0.63810.h5\n",
      "Epoch 12/35\n",
      "45/45 [==============================] - 41s 917ms/step - loss: 0.4133 - categorical_accuracy: 0.8770 - val_loss: 0.8521 - val_categorical_accuracy: 0.6571\n",
      "\n",
      "Epoch 00012: saving model to model_init_2020-03-0111_08_09.236698/model-00012-0.41331-0.87704-0.85205-0.65714.h5\n",
      "Epoch 13/35\n",
      "45/45 [==============================] - 41s 903ms/step - loss: 0.3919 - categorical_accuracy: 0.8667 - val_loss: 0.8171 - val_categorical_accuracy: 0.7048\n",
      "\n",
      "Epoch 00013: saving model to model_init_2020-03-0111_08_09.236698/model-00013-0.39191-0.86667-0.81707-0.70476.h5\n",
      "Epoch 14/35\n",
      "45/45 [==============================] - 42s 941ms/step - loss: 0.3629 - categorical_accuracy: 0.8933 - val_loss: 0.8002 - val_categorical_accuracy: 0.7048\n",
      "\n",
      "Epoch 00014: saving model to model_init_2020-03-0111_08_09.236698/model-00014-0.36291-0.89333-0.80025-0.70476.h5\n",
      "Epoch 15/35\n",
      "45/45 [==============================] - 42s 930ms/step - loss: 0.3748 - categorical_accuracy: 0.8830 - val_loss: 1.3017 - val_categorical_accuracy: 0.6190\n",
      "\n",
      "Epoch 00015: saving model to model_init_2020-03-0111_08_09.236698/model-00015-0.37476-0.88296-1.30171-0.61905.h5\n",
      "Epoch 16/35\n",
      "45/45 [==============================] - 41s 904ms/step - loss: 0.3158 - categorical_accuracy: 0.9170 - val_loss: 1.4076 - val_categorical_accuracy: 0.5333\n",
      "\n",
      "Epoch 00016: saving model to model_init_2020-03-0111_08_09.236698/model-00016-0.31582-0.91704-1.40756-0.53333.h5\n",
      "Epoch 17/35\n",
      "45/45 [==============================] - 42s 933ms/step - loss: 0.2497 - categorical_accuracy: 0.9348 - val_loss: 0.9177 - val_categorical_accuracy: 0.7238\n",
      "\n",
      "Epoch 00017: saving model to model_init_2020-03-0111_08_09.236698/model-00017-0.24970-0.93481-0.91772-0.72381.h5\n",
      "Epoch 18/35\n",
      "45/45 [==============================] - 40s 896ms/step - loss: 0.2629 - categorical_accuracy: 0.9289 - val_loss: 1.2104 - val_categorical_accuracy: 0.5810\n",
      "\n",
      "Epoch 00018: saving model to model_init_2020-03-0111_08_09.236698/model-00018-0.26290-0.92889-1.21045-0.58095.h5\n",
      "Epoch 19/35\n",
      "45/45 [==============================] - 41s 922ms/step - loss: 0.2502 - categorical_accuracy: 0.9407 - val_loss: 0.9082 - val_categorical_accuracy: 0.6667\n",
      "\n",
      "Epoch 00019: saving model to model_init_2020-03-0111_08_09.236698/model-00019-0.25018-0.94074-0.90820-0.66667.h5\n",
      "Epoch 20/35\n",
      "45/45 [==============================] - 41s 916ms/step - loss: 0.2235 - categorical_accuracy: 0.9378 - val_loss: 0.8633 - val_categorical_accuracy: 0.7238\n",
      "\n",
      "Epoch 00020: saving model to model_init_2020-03-0111_08_09.236698/model-00020-0.22353-0.93778-0.86332-0.72381.h5\n",
      "Epoch 21/35\n",
      "45/45 [==============================] - 41s 916ms/step - loss: 0.2781 - categorical_accuracy: 0.9200 - val_loss: 1.3204 - val_categorical_accuracy: 0.5619\n",
      "\n",
      "Epoch 00021: saving model to model_init_2020-03-0111_08_09.236698/model-00021-0.27813-0.92000-1.32038-0.56190.h5\n",
      "Epoch 22/35\n",
      "45/45 [==============================] - 42s 941ms/step - loss: 0.2508 - categorical_accuracy: 0.9304 - val_loss: 1.9787 - val_categorical_accuracy: 0.5619\n",
      "\n",
      "Epoch 00022: saving model to model_init_2020-03-0111_08_09.236698/model-00022-0.25083-0.93037-1.97869-0.56190.h5\n",
      "Epoch 23/35\n",
      "45/45 [==============================] - 40s 892ms/step - loss: 0.2085 - categorical_accuracy: 0.9437 - val_loss: 1.1799 - val_categorical_accuracy: 0.6000\n",
      "\n",
      "Epoch 00023: saving model to model_init_2020-03-0111_08_09.236698/model-00023-0.20851-0.94370-1.17992-0.60000.h5\n",
      "Epoch 24/35\n",
      "45/45 [==============================] - 42s 928ms/step - loss: 0.2168 - categorical_accuracy: 0.9289 - val_loss: 1.1505 - val_categorical_accuracy: 0.5810\n",
      "\n",
      "Epoch 00024: saving model to model_init_2020-03-0111_08_09.236698/model-00024-0.21684-0.92889-1.15047-0.58095.h5\n",
      "Epoch 25/35\n",
      "45/45 [==============================] - 40s 899ms/step - loss: 0.1704 - categorical_accuracy: 0.9526 - val_loss: 1.3610 - val_categorical_accuracy: 0.5429\n",
      "\n",
      "Epoch 00025: saving model to model_init_2020-03-0111_08_09.236698/model-00025-0.17042-0.95259-1.36095-0.54286.h5\n",
      "Epoch 26/35\n",
      "45/45 [==============================] - 41s 905ms/step - loss: 0.1778 - categorical_accuracy: 0.9467 - val_loss: 0.8629 - val_categorical_accuracy: 0.6952\n",
      "\n",
      "Epoch 00026: saving model to model_init_2020-03-0111_08_09.236698/model-00026-0.17777-0.94667-0.86290-0.69524.h5\n",
      "Epoch 27/35\n",
      "45/45 [==============================] - 42s 935ms/step - loss: 0.1244 - categorical_accuracy: 0.9733 - val_loss: 0.9964 - val_categorical_accuracy: 0.6952\n",
      "\n",
      "Epoch 00027: saving model to model_init_2020-03-0111_08_09.236698/model-00027-0.12443-0.97333-0.99643-0.69524.h5\n",
      "Epoch 28/35\n",
      "45/45 [==============================] - 40s 887ms/step - loss: 0.1668 - categorical_accuracy: 0.9541 - val_loss: 0.6858 - val_categorical_accuracy: 0.8000\n",
      "\n",
      "Epoch 00028: saving model to model_init_2020-03-0111_08_09.236698/model-00028-0.16685-0.95407-0.68576-0.80000.h5\n",
      "Epoch 29/35\n",
      "45/45 [==============================] - 42s 942ms/step - loss: 0.1691 - categorical_accuracy: 0.9615 - val_loss: 0.6686 - val_categorical_accuracy: 0.7714\n",
      "\n",
      "Epoch 00029: saving model to model_init_2020-03-0111_08_09.236698/model-00029-0.16915-0.96148-0.66864-0.77143.h5\n",
      "Epoch 30/35\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45/45 [==============================] - 40s 896ms/step - loss: 0.1442 - categorical_accuracy: 0.9556 - val_loss: 1.0057 - val_categorical_accuracy: 0.6190\n",
      "\n",
      "Epoch 00030: saving model to model_init_2020-03-0111_08_09.236698/model-00030-0.14423-0.95556-1.00572-0.61905.h5\n",
      "Epoch 31/35\n",
      "45/45 [==============================] - 41s 921ms/step - loss: 0.1176 - categorical_accuracy: 0.9793 - val_loss: 1.1551 - val_categorical_accuracy: 0.5524\n",
      "\n",
      "Epoch 00031: saving model to model_init_2020-03-0111_08_09.236698/model-00031-0.11763-0.97926-1.15507-0.55238.h5\n",
      "Epoch 32/35\n",
      "45/45 [==============================] - 42s 930ms/step - loss: 0.1126 - categorical_accuracy: 0.9748 - val_loss: 1.1565 - val_categorical_accuracy: 0.7048\n",
      "\n",
      "Epoch 00032: saving model to model_init_2020-03-0111_08_09.236698/model-00032-0.11257-0.97481-1.15649-0.70476.h5\n",
      "Epoch 33/35\n",
      "45/45 [==============================] - 41s 910ms/step - loss: 0.0774 - categorical_accuracy: 0.9867 - val_loss: 0.9198 - val_categorical_accuracy: 0.7524\n",
      "\n",
      "Epoch 00033: saving model to model_init_2020-03-0111_08_09.236698/model-00033-0.07745-0.98667-0.91979-0.75238.h5\n",
      "Epoch 34/35\n",
      "45/45 [==============================] - 41s 917ms/step - loss: 0.1062 - categorical_accuracy: 0.9793 - val_loss: 1.6395 - val_categorical_accuracy: 0.4476\n",
      "\n",
      "Epoch 00034: saving model to model_init_2020-03-0111_08_09.236698/model-00034-0.10616-0.97926-1.63951-0.44762.h5\n",
      "Epoch 35/35\n",
      "45/45 [==============================] - 42s 932ms/step - loss: 0.1032 - categorical_accuracy: 0.9719 - val_loss: 1.0353 - val_categorical_accuracy: 0.6571\n",
      "\n",
      "Epoch 00035: saving model to model_init_2020-03-0111_08_09.236698/model-00035-0.10323-0.97185-1.03531-0.65714.h5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f8fba2aab00>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Model Generation\n",
    "model.fit_generator(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1, \n",
    "                    callbacks=callbacks_list, validation_data=val_generator, \n",
    "                    validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We got best model at 28 epoch with 80% test accuracy and 95% train accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Below is the LSTM Model we have tried which results in 77% test accuracy and 98% train accuracy\n",
    "# model_2D = Sequential()\n",
    "\n",
    "# model_2D.add(Conv2D(8, (3, 3), padding='same', input_shape=(100,100,3)))\n",
    "# model_2D.add(Activation('relu')) \n",
    "# model_2D.add(BatchNormalization())\n",
    "# model_2D.add(MaxPooling2D(pool_size=(2,2),strides=(2,2)))\n",
    "\n",
    "# model_2D.add(Conv2D(16, (3, 3), padding='same'))\n",
    "# model_2D.add(Activation('relu')) \n",
    "# model_2D.add(BatchNormalization())\n",
    "# model_2D.add(MaxPooling2D(pool_size=(2,2),strides=(2,2)))\n",
    "\n",
    "# model_2D.add(Conv2D(32, (3, 3), padding='same'))\n",
    "# model_2D.add(Activation('relu'))\n",
    "# model_2D.add(BatchNormalization())\n",
    "# model_2D.add(MaxPooling2D(pool_size=(2,2),strides=(2,2)))\n",
    "\n",
    "# model_2D.add(Conv2D(64, (3, 3), padding='same'))\n",
    "# model_2D.add(Activation('relu'))\n",
    "# model_2D.add(BatchNormalization())\n",
    "# model_2D.add(MaxPooling2D(pool_size=(2,2),strides=(2,2)))\n",
    "\n",
    "# model_2D.add(Dropout(0.5))\n",
    "# model_2D.add(Flatten())\n",
    "\n",
    "# model_2D.add(Dense(64, activation='relu'))\n",
    "# model_2D.add(BatchNormalization())\n",
    "# model_2D.add(Dropout(0.5))\n",
    "\n",
    "# model_TD = Sequential()\n",
    "# model_TD.add(TimeDistributed(model_2D, input_shape=(15, 100, 100,3)))\n",
    "# model_TD.add(LSTM(64))\n",
    "# model_TD.add(Dense(5, activation='softmax'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
